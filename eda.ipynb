{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import my_globals\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1986304945</td>\n",
       "      <td>Sun May 31 18:38:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kjgriffin18</td>\n",
       "      <td>Omfggg rob patterison won again  fuck yes!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2253630799</td>\n",
       "      <td>Sat Jun 20 08:29:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>chloefletch23</td>\n",
       "      <td>Defs shouldnt have wore heels last night. Now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1979545392</td>\n",
       "      <td>Sun May 31 03:30:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>groovychick3290</td>\n",
       "      <td>@NB82 lol nope im housebound all wk til friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1963101679</td>\n",
       "      <td>Fri May 29 12:12:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jusjuhi</td>\n",
       "      <td>my new dress looks sort of...horrible   http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2228651766</td>\n",
       "      <td>Thu Jun 18 14:42:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cizzln</td>\n",
       "      <td>or they just picked the#squarespace winner lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>1832774127</td>\n",
       "      <td>Sun May 17 21:28:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>deadpresident</td>\n",
       "      <td>Market gained 15% , my portfolio gained only 12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>2285120770</td>\n",
       "      <td>Mon Jun 22 14:44:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>phrakonline</td>\n",
       "      <td>damn i feel terrible...like death warmed up  i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>4</td>\n",
       "      <td>2063802433</td>\n",
       "      <td>Sun Jun 07 04:08:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ireckon</td>\n",
       "      <td>@KirstyWrites @ajaxive i washed my makeup off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>2064078409</td>\n",
       "      <td>Sun Jun 07 05:08:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CrunchBytes</td>\n",
       "      <td>Beeb not showing in car footage on iplayer.  R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4</td>\n",
       "      <td>1793351877</td>\n",
       "      <td>Thu May 14 02:21:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sedatedtimes</td>\n",
       "      <td>@beenieweenie Crap! I forgot today was Thursda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target         ids                          date      flag  \\\n",
       "0         4  1986304945  Sun May 31 18:38:39 PDT 2009  NO_QUERY   \n",
       "1         0  2253630799  Sat Jun 20 08:29:31 PDT 2009  NO_QUERY   \n",
       "2         0  1979545392  Sun May 31 03:30:31 PDT 2009  NO_QUERY   \n",
       "3         0  1963101679  Fri May 29 12:12:19 PDT 2009  NO_QUERY   \n",
       "4         0  2228651766  Thu Jun 18 14:42:47 PDT 2009  NO_QUERY   \n",
       "..      ...         ...                           ...       ...   \n",
       "995       0  1832774127  Sun May 17 21:28:14 PDT 2009  NO_QUERY   \n",
       "996       0  2285120770  Mon Jun 22 14:44:00 PDT 2009  NO_QUERY   \n",
       "997       4  2063802433  Sun Jun 07 04:08:00 PDT 2009  NO_QUERY   \n",
       "998       0  2064078409  Sun Jun 07 05:08:55 PDT 2009  NO_QUERY   \n",
       "999       4  1793351877  Thu May 14 02:21:33 PDT 2009  NO_QUERY   \n",
       "\n",
       "                user                                               text  \n",
       "0        kjgriffin18         Omfggg rob patterison won again  fuck yes!  \n",
       "1      chloefletch23  Defs shouldnt have wore heels last night. Now ...  \n",
       "2    groovychick3290  @NB82 lol nope im housebound all wk til friday...  \n",
       "3            jusjuhi  my new dress looks sort of...horrible   http:/...  \n",
       "4             cizzln  or they just picked the#squarespace winner lik...  \n",
       "..               ...                                                ...  \n",
       "995    deadpresident  Market gained 15% , my portfolio gained only 12%   \n",
       "996      phrakonline  damn i feel terrible...like death warmed up  i...  \n",
       "997          ireckon  @KirstyWrites @ajaxive i washed my makeup off ...  \n",
       "998      CrunchBytes  Beeb not showing in car footage on iplayer.  R...  \n",
       "999     sedatedtimes  @beenieweenie Crap! I forgot today was Thursda...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_sub_dataset\n",
    "\n",
    "size = 1000,\n",
    "random_seed = 5\n",
    "get_sub_dataset(size = size, random_seed=random_seed)\n",
    "data = pd.read_csv(my_globals.DATA_DIR + f\"/twitter_seed{random_seed}.csv\")\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing \n",
    "- Punctuation deletion\n",
    "- Stopwords deletion\n",
    "- Digits deletion\n",
    "- Link deletion\n",
    "- Decontraction\n",
    "- Lemmetization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_globals\n",
    "from typing import List\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def setup_nltk():\n",
    "    \"\"\"Downloads necessary packages within nltk\"\"\"\n",
    "    packages = [\"punkt\", \"wordnet\", \"stopwords\"]\n",
    "    for p in packages:\n",
    "        try:\n",
    "            nltk.data.find(p)\n",
    "        except LookupError:\n",
    "            nltk.download(p)\n",
    "\n",
    "\n",
    "def tokenize(s: str, how=\"word_tokenize\") -> List[str]:\n",
    "    if how == \"word_tokenize\":\n",
    "        return word_tokenize(s)\n",
    "    elif how == \"split\":\n",
    "        return s.split()\n",
    "\n",
    "\n",
    "def del_username(s: str) -> str:\n",
    "    \"\"\"Delete @Username from a tweet str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([t for t in tokenize(s, how=\"split\") if not t.startswith(\"@\")])\n",
    "\n",
    "\n",
    "def del_punc(s: str) -> str:\n",
    "    \"\"\"Delete punctuations from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    punc = my_globals.PUNCS\n",
    "    return \"\".join([w for w in s if w not in punc])\n",
    "\n",
    "\n",
    "def del_link(s: str) -> str:\n",
    "    \"\"\"Delete links from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    r = r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)\"\n",
    "    return \" \".join([re.sub(r, \"\", t) for t in tokenize(s, how=\"split\")])\n",
    "\n",
    "\n",
    "def decontract(s: str) -> str:\n",
    "    \"\"\"Remove contractions in text.\n",
    "    e.g. I'm -> I am; she'd -> she would\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for t in tokenize(s, how=\"split\"):\n",
    "        tokens.append(contractions.fix(t))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def del_stopwords(s: str) -> str:\n",
    "    \"\"\"Delete stopwords and punctuation from a string.\n",
    "    Note that the type-hinting indicates that this function ought\n",
    "    to be run first in the pre-processing pipeline.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    return \" \".join([t for t in tokenize(s) if t not in stop_words])\n",
    "\n",
    "\n",
    "def del_digits(s: str) -> str:\n",
    "    \"\"\"Delete digits from str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    return \" \".join([w for w in tokenize(s) if not w.isdigit()])\n",
    "\n",
    "\n",
    "def lemmatize(s: str) -> str:\n",
    "    \"\"\"Lemmatize str.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(t) for t in tokenize(s)])\n",
    "\n",
    "\n",
    "def preprocess_pipeline(s: str) -> str:\n",
    "    \"\"\"Run string through all pre-processing functions.\n",
    "\n",
    "    :param s: input string\n",
    "    :type s: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    s = reduce(\n",
    "        lambda value, function: function(value),\n",
    "        (\n",
    "            del_link,\n",
    "            del_username,\n",
    "            decontract,\n",
    "            lemmatize,\n",
    "            del_stopwords,\n",
    "            del_punc,\n",
    "            del_digits\n",
    "        ),\n",
    "        s,\n",
    "    )\n",
    "\n",
    "    return s.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "def str_datetime(s: str):\n",
    "    \"\"\"Parse and format a datetime str to weekday and datetime.MAXYEAR\n",
    "    \n",
    "    :param s: input string containing datetime information\n",
    "    :type s: str\n",
    "    :rtype: tuple[str, str]\n",
    "    \"\"\"\n",
    "    ss = parse(s).strftime('%a %Y-%m-%d %H:%M:%S')\n",
    "    return ss[:3], ss[4:]\n",
    "\n",
    "\n",
    "def cleaning(df: pd.DataFrame):\n",
    "    \"\"\"Cleaning script of the data (or subset).\n",
    "    \n",
    "    :param df: input dataframe.\n",
    "    :type df: pd.DataFrame\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Parse weekday and datetime\n",
    "    weekday_datetime = pd.DataFrame(\n",
    "        list(df.loc[:, \"date\"].apply(str_datetime)),\n",
    "        columns=[\"weekday\", \"datetime\"]\n",
    "    )\n",
    "    # One-hot encode weekday\n",
    "    weekdaydummies = pd.get_dummies(\n",
    "        weekday_datetime['weekday'], \n",
    "        prefix='weekday', \n",
    "        dtype=float\n",
    "    )\n",
    "    weekdaydummies = pd.DataFrame(\n",
    "        weekdaydummies, \n",
    "        columns=['weekday_'+w for w in [\n",
    "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"\n",
    "        ]]\n",
    "    )\n",
    "    # Concatenate weekday dummies to other features\n",
    "    weekdaydummies_datetime = pd.concat(\n",
    "        [weekdaydummies, weekday_datetime['datetime']], \n",
    "        axis=1\n",
    "    )\n",
    "    df = pd.concat([df, weekdaydummies_datetime], axis=1)\n",
    "    # Drop the column with single unique value.\n",
    "    df.drop(\"flag\", axis = 1, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linru\\AppData\\Roaming\\Python\\Python39\\site-packages\\dateutil\\parser\\_parser.py:1213: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>weekday_Mon</th>\n",
       "      <th>weekday_Tue</th>\n",
       "      <th>weekday_Wed</th>\n",
       "      <th>weekday_Thu</th>\n",
       "      <th>weekday_Fri</th>\n",
       "      <th>weekday_Sat</th>\n",
       "      <th>weekday_Sun</th>\n",
       "      <th>datetime</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1986304945</td>\n",
       "      <td>Sun May 31 18:38:39 PDT 2009</td>\n",
       "      <td>kjgriffin18</td>\n",
       "      <td>Omfggg rob patterison won again  fuck yes!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2009-05-31 18:38:39</td>\n",
       "      <td>omfggg rob patterison fuck yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2253630799</td>\n",
       "      <td>Sat Jun 20 08:29:31 PDT 2009</td>\n",
       "      <td>chloefletch23</td>\n",
       "      <td>Defs shouldnt have wore heels last night. Now ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-06-20 08:29:31</td>\n",
       "      <td>defs wore heel last night now knee playing lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1979545392</td>\n",
       "      <td>Sun May 31 03:30:31 PDT 2009</td>\n",
       "      <td>groovychick3290</td>\n",
       "      <td>@NB82 lol nope im housebound all wk til friday...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2009-05-31 03:30:31</td>\n",
       "      <td>lol nope housebound wk til friday weather chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1963101679</td>\n",
       "      <td>Fri May 29 12:12:19 PDT 2009</td>\n",
       "      <td>jusjuhi</td>\n",
       "      <td>my new dress looks sort of...horrible   http:/...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-05-29 12:12:19</td>\n",
       "      <td>new dress look sort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2228651766</td>\n",
       "      <td>Thu Jun 18 14:42:47 PDT 2009</td>\n",
       "      <td>cizzln</td>\n",
       "      <td>or they just picked the#squarespace winner lik...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-06-18 14:42:47</td>\n",
       "      <td>picked squarespace winner like min ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>1832774127</td>\n",
       "      <td>Sun May 17 21:28:14 PDT 2009</td>\n",
       "      <td>deadpresident</td>\n",
       "      <td>Market gained 15% , my portfolio gained only 12%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2009-05-17 21:28:14</td>\n",
       "      <td>market gained portfolio gained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>2285120770</td>\n",
       "      <td>Mon Jun 22 14:44:00 PDT 2009</td>\n",
       "      <td>phrakonline</td>\n",
       "      <td>damn i feel terrible...like death warmed up  i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-06-22 14:44:00</td>\n",
       "      <td>damn feel death warmed hope horrible cold go a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>4</td>\n",
       "      <td>2063802433</td>\n",
       "      <td>Sun Jun 07 04:08:00 PDT 2009</td>\n",
       "      <td>ireckon</td>\n",
       "      <td>@KirstyWrites @ajaxive i washed my makeup off ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2009-06-07 04:08:00</td>\n",
       "      <td>washed makeup look happened queen s birthday look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>2064078409</td>\n",
       "      <td>Sun Jun 07 05:08:55 PDT 2009</td>\n",
       "      <td>CrunchBytes</td>\n",
       "      <td>Beeb not showing in car footage on iplayer.  R...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2009-06-07 05:08:55</td>\n",
       "      <td>beeb showing car footage iplayer rb stuck behi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4</td>\n",
       "      <td>1793351877</td>\n",
       "      <td>Thu May 14 02:21:33 PDT 2009</td>\n",
       "      <td>sedatedtimes</td>\n",
       "      <td>@beenieweenie Crap! I forgot today was Thursda...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2009-05-14 02:21:33</td>\n",
       "      <td>crap i forgot today wa thursday reason i wa th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target         ids                          date             user  \\\n",
       "0         4  1986304945  Sun May 31 18:38:39 PDT 2009      kjgriffin18   \n",
       "1         0  2253630799  Sat Jun 20 08:29:31 PDT 2009    chloefletch23   \n",
       "2         0  1979545392  Sun May 31 03:30:31 PDT 2009  groovychick3290   \n",
       "3         0  1963101679  Fri May 29 12:12:19 PDT 2009          jusjuhi   \n",
       "4         0  2228651766  Thu Jun 18 14:42:47 PDT 2009           cizzln   \n",
       "..      ...         ...                           ...              ...   \n",
       "995       0  1832774127  Sun May 17 21:28:14 PDT 2009    deadpresident   \n",
       "996       0  2285120770  Mon Jun 22 14:44:00 PDT 2009      phrakonline   \n",
       "997       4  2063802433  Sun Jun 07 04:08:00 PDT 2009          ireckon   \n",
       "998       0  2064078409  Sun Jun 07 05:08:55 PDT 2009      CrunchBytes   \n",
       "999       4  1793351877  Thu May 14 02:21:33 PDT 2009     sedatedtimes   \n",
       "\n",
       "                                                  text  weekday_Mon  \\\n",
       "0           Omfggg rob patterison won again  fuck yes!          0.0   \n",
       "1    Defs shouldnt have wore heels last night. Now ...          0.0   \n",
       "2    @NB82 lol nope im housebound all wk til friday...          0.0   \n",
       "3    my new dress looks sort of...horrible   http:/...          0.0   \n",
       "4    or they just picked the#squarespace winner lik...          0.0   \n",
       "..                                                 ...          ...   \n",
       "995  Market gained 15% , my portfolio gained only 12%           0.0   \n",
       "996  damn i feel terrible...like death warmed up  i...          1.0   \n",
       "997  @KirstyWrites @ajaxive i washed my makeup off ...          0.0   \n",
       "998  Beeb not showing in car footage on iplayer.  R...          0.0   \n",
       "999  @beenieweenie Crap! I forgot today was Thursda...          0.0   \n",
       "\n",
       "     weekday_Tue  weekday_Wed  weekday_Thu  weekday_Fri  weekday_Sat  \\\n",
       "0            0.0          0.0          0.0          0.0          0.0   \n",
       "1            0.0          0.0          0.0          0.0          1.0   \n",
       "2            0.0          0.0          0.0          0.0          0.0   \n",
       "3            0.0          0.0          0.0          1.0          0.0   \n",
       "4            0.0          0.0          1.0          0.0          0.0   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "995          0.0          0.0          0.0          0.0          0.0   \n",
       "996          0.0          0.0          0.0          0.0          0.0   \n",
       "997          0.0          0.0          0.0          0.0          0.0   \n",
       "998          0.0          0.0          0.0          0.0          0.0   \n",
       "999          0.0          0.0          1.0          0.0          0.0   \n",
       "\n",
       "     weekday_Sun             datetime  \\\n",
       "0            1.0  2009-05-31 18:38:39   \n",
       "1            0.0  2009-06-20 08:29:31   \n",
       "2            1.0  2009-05-31 03:30:31   \n",
       "3            0.0  2009-05-29 12:12:19   \n",
       "4            0.0  2009-06-18 14:42:47   \n",
       "..           ...                  ...   \n",
       "995          1.0  2009-05-17 21:28:14   \n",
       "996          0.0  2009-06-22 14:44:00   \n",
       "997          1.0  2009-06-07 04:08:00   \n",
       "998          1.0  2009-06-07 05:08:55   \n",
       "999          0.0  2009-05-14 02:21:33   \n",
       "\n",
       "                                        processed_text  \n",
       "0                       omfggg rob patterison fuck yes  \n",
       "1    defs wore heel last night now knee playing lik...  \n",
       "2    lol nope housebound wk til friday weather chan...  \n",
       "3                                  new dress look sort  \n",
       "4               picked squarespace winner like min ago  \n",
       "..                                                 ...  \n",
       "995                     market gained portfolio gained  \n",
       "996  damn feel death warmed hope horrible cold go a...  \n",
       "997  washed makeup look happened queen s birthday look  \n",
       "998  beeb showing car footage iplayer rb stuck behi...  \n",
       "999  crap i forgot today wa thursday reason i wa th...  \n",
       "\n",
       "[1000 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(my_globals.DATA_DIR + \"/twitter_seed5.csv\")\n",
    "data = cleaning(data)\n",
    "data[\"processed_text\"] = data[\"text\"].apply(preprocess_pipeline)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2857)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "X = vectorizer.fit_transform(data[\"processed_text\"])\n",
    "X = tfidf.fit_transform(X)\n",
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
